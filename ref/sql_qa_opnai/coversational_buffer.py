# -*- coding: utf-8 -*-
"""LLama2Demo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f5uCFFjIdz76gw25Vdw54VyK7j96CgCT
"""

!pip install -q transformers einops accelerate langchain bitsandbytes

!huggingface-cli login

!pip install sentencepiece

from langchain import HuggingFacePipeline
from transformers import AutoTokenizer
import transformers
import torch

model = "meta-llama/Llama-2-13b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model)

pipeline = transformers.pipeline(
    "text-generation", #task
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map="auto",
    max_length=1000,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id
)

!pip install langchain -q

llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0.2,'top_p':0.1,'max_tokens' : 256})

from langchain.chains.conversation.memory import ConversationBufferMemory
from langchain.chains import ConversationChain
memory = ConversationBufferMemory()

conversation = ConversationChain(
    llm=llm,
    verbose=True,
    memory=memory
)

conversation.predict(input="Hi there! I am Sam")

print(conversation.memory.buffer)

from langchain.chains.conversation.memory import ConversationBufferWindowMemory
from langchain.chains import ConversationChain

window_memory = ConversationBufferWindowMemory(k=2)

conversation = ConversationChain(
    llm=llm,
    verbose=True,
    memory=window_memory
)

conversation.predict(input="Hi there! I am Sam")
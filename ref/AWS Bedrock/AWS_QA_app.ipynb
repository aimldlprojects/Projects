{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8917bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.embeddings.bedrock import BedrockEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.vectorstores import FAISS\n",
    "import streamlit as st\n",
    "\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed88a08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.config import Config\n",
    "# from retriver import load_local_vector_store\n",
    "\n",
    "\n",
    "retry_config = Config(\n",
    "        region_name = 'us-east-1',\n",
    "        retries = {\n",
    "            'max_attempts': 10,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    ")\n",
    "bedrock = boto3.client(\n",
    "    aws_access_key_id=\"access_key\",\n",
    "    aws_secret_access_key=\"secret_key\",\n",
    "    service_name='bedrock-runtime', \n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "#print(response)\n",
    "\n",
    "model_id = \"amazon.titan-embed-text-v1\"\n",
    "content_type = \"application/json\"\n",
    "accept = \"*/*\"\n",
    "#body = \"{\\\"prompt\\\":\\\"\\\",\\\"maxTokens\\\":4323,\\\"temperature\\\":0.7,\\\"topP\\\":1,\\\"stopSequences\\\":[],\\\"countPenalty\\\":{\\\"scale\\\":0},\\\"presencePenalty\\\":{\\\"scale\\\":0},\\\"frequencyPenalty\\\":{\\\"scale\\\":0}}\" \\\n",
    "\n",
    "\n",
    "# Creating boto3 session by passing profile information. Profile can be parametrized depeding upon the env you are using\n",
    "session = boto3.session.Session(profile_name='test-demo')\n",
    "\n",
    "\"\"\"\" \n",
    "btot3 provides two different client to ivoke bedrock operation.\n",
    "1. bedrock : creating and managing Bedrock models.\n",
    "2. bedrock-runtime : Running inference using Bedrock models.\n",
    "\"\"\"\n",
    "boto3_bedrock = session.client(\"bedrock\", config=retry_config)\n",
    "boto3_bedrock_runtime = session.client(\"bedrock-runtime\", config=retry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e69bfbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We will implement RAG architecture. The goal is to build vector store (Knowedge base to reduce hallucinations) \n",
    "so that model can refer to data we have provided.\n",
    "\n",
    "To achieve this, we need to first source data (this can be archived PDF/docs/txt/csv/anyother datastore even sql tables) \n",
    "So the pipeline will be.\n",
    " 1. Source datasets.\n",
    " 2. Update If any transformation required. \n",
    " 3. Split and create chunks. [Used in NLP. It requires optimization to get  better output.]\n",
    " 4. Create embedding using embedding modules [Can be used various modules available]\n",
    "'''\n",
    "\n",
    "EMBEDDINGS_MODEL_ID='amazon.titan-embed-text-v1'\n",
    "\n",
    "brrkEmbeddings = BedrockEmbeddings(model_id=EMBEDDINGS_MODEL_ID,client=boto3_bedrock_runtime,)\n",
    "bedrock_llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock_runtime )\n",
    "#bedrock_llm = Bedrock(model_id=\"ai21.j2-ultra-v1\", client=boto3_bedrock_runtime, model_kwargs={'max_tokens_to_sample':1000})\n",
    "\n",
    "save_local_vector_store_path='C:\\\\Users\\\\RT/vectorstore/14112023235757.vs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33c97cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store\n",
      "<langchain.vectorstores.faiss.FAISS object at 0x00000263137C6D50>\n"
     ]
    }
   ],
   "source": [
    "def load_local_vector_store(vector_store_path):\n",
    "    try:\n",
    "        with open(f\"{vector_store_path}/embeddings_model_id\", 'r') as f:\n",
    "            embeddings_model_id = f.read()\n",
    "        vector_store = FAISS.load_local(vector_store_path, brrkEmbeddings)\n",
    "        print(\"Loaded vector store\")\n",
    "        return vector_store\n",
    "    except Exception:\n",
    "        print(\"Failed to load vector store, continuing creating one...\")\n",
    "        \n",
    "print(load_local_vector_store(save_local_vector_store_path))\n",
    "\n",
    "prompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer to\n",
    "the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "'''prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\",\"max_token_size\" == 2000]\n",
    ")'''\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bac053c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 13:57:55.705 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\anaconda\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nWe will implement RAG architecture. The goal is to build vector store (Knowedge base to reduce hallucinations) \\nso that model can refer to data we have provided.\\n\\nTo achieve this, we need to first source data (this can be archived PDF/docs/txt/csv/anyother datastore even sql tables) \\nSo the pipeline will be.\\n 1. Source datasets.\\n 2. Update If any transformation required. \\n 3. Split and create chunks. [Used in NLP. It requires optimization to get  better output.]\\n 4. Create embedding using embedding modules [Can be used various modules available]\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = st.text_area(\"Enter Text To summarize\")\n",
    "button = st.button(\"Generate Summary\")\n",
    "if user_input and button:\n",
    "    summary = create_RetrievalQA_chain(user_input)\n",
    "    st.write(\"Summary : \", summary)\n",
    "\n",
    "from botocore.config import Config\n",
    "# from retriver import load_local_vector_store\n",
    "\n",
    "\n",
    "retry_config = Config(\n",
    "        region_name = 'us-east-1',\n",
    "        retries = {\n",
    "            'max_attempts': 10,\n",
    "            'mode': 'standard'\n",
    "        }\n",
    ")\n",
    "\n",
    "# Creating boto3 session by passing profile information. Profile can be parametrized depeding upon the env you are using\n",
    "session = boto3.session.Session(profile_name='test-demo')\n",
    "\n",
    "\"\"\"\" \n",
    "btot3 provides two different client to ivoke bedrock operation.\n",
    "1. bedrock : creating and managing Bedrock models.\n",
    "2. bedrock-runtime : Running inference using Bedrock models.\n",
    "\"\"\"\n",
    "boto3_bedrock = session.client(\"bedrock\", config=retry_config)\n",
    "boto3_bedrock_runtime = session.client(\"bedrock-runtime\", config=retry_config)\n",
    "\n",
    "\n",
    "'''\n",
    "We will implement RAG architecture. The goal is to build vector store (Knowedge base to reduce hallucinations) \n",
    "so that model can refer to data we have provided.\n",
    "\n",
    "To achieve this, we need to first source data (this can be archived PDF/docs/txt/csv/anyother datastore even sql tables) \n",
    "So the pipeline will be.\n",
    " 1. Source datasets.\n",
    " 2. Update If any transformation required. \n",
    " 3. Split and create chunks. [Used in NLP. It requires optimization to get  better output.]\n",
    " 4. Create embedding using embedding modules [Can be used various modules available]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f58d8aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store\n",
      "<langchain.vectorstores.faiss.FAISS object at 0x000002631426FB50>\n"
     ]
    }
   ],
   "source": [
    "EMBEDDINGS_MODEL_ID='amazon.titan-embed-text-v1'\n",
    "brrkEmbeddings = BedrockEmbeddings(model_id=EMBEDDINGS_MODEL_ID,client=boto3_bedrock_runtime,)\n",
    "bedrock_llm = Bedrock(model_id=\"anthropic.claude-v2\", client=boto3_bedrock_runtime )\n",
    "#bedrock_llm = Bedrock(model_id=\"ai21.j2-ultra-v1\", client=boto3_bedrock_runtime, model_kwargs={'max_tokens_to_sample':1000})\n",
    "\n",
    "save_local_vector_store_path='C:\\\\Users\\\\RT/vectorstore/14112023235757.vs'\n",
    "\n",
    "def load_local_vector_store(vector_store_path):\n",
    "    try:\n",
    "        with open(f\"{vector_store_path}/embeddings_model_id\", 'r') as f:\n",
    "            embeddings_model_id = f.read()\n",
    "        vector_store = FAISS.load_local(vector_store_path, brrkEmbeddings)\n",
    "        print(\"Loaded vector store\")\n",
    "        return vector_store\n",
    "    except Exception:\n",
    "        print(\"Failed to load vector store, continuing creating one...\")\n",
    "        \n",
    "print(load_local_vector_store(save_local_vector_store_path))\n",
    "\n",
    "prompt_template = \"\"\"Human: Use the following pieces of context to provide a concise answer to\n",
    "the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Assistant:\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4806ae44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_RetrievalQA_chain(query):\n",
    "    #print(\"Connecting to bedrock\")\n",
    "    vector_store = load_local_vector_store(save_local_vector_store_path)\n",
    "    #print(\"this is vector store\", vector_store)\n",
    "    retriever = vector_store.as_retriever(\n",
    "        search_type=\"similarity\", search_kwargs={\"k\": 5, \"include_metadata\": True}\n",
    "    )\n",
    "    #print(\"this is retriever\", retriever)\n",
    "    chain = RetrievalQA.from_chain_type(\n",
    "        llm=bedrock_llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": prompt}\n",
    "    )\n",
    "    #print(\"this is chain\", chain)\n",
    "    result = chain({\"query\": query})\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8357bc6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded vector store\n",
      " Based on the information provided, it does not specify which specific LLMs are used for chatbots. The passage discusses general categories of LLMs like GPT models developed by OpenAI, as well as Transformers developed by Google. But it does not mention any LLMs specifically built or optimized for chatbots. So I don't have enough context here to determine which LLMs are specifically used for chatbots. The information focuses more on the history and architectures of LLMs in general.\n"
     ]
    }
   ],
   "source": [
    "query = \"which llms used for chat bot\"\n",
    "result=create_RetrievalQA_chain(query)\n",
    "\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf58b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = st.text_area(\"Enter Text To summarize\")\n",
    "button = st.button(\"Generate Summary\")\n",
    "if user_input and button:\n",
    "    summary = create_RetrievalQA_chain(user_input)\n",
    "    st.write(\"Summary : \", summary)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b742cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fbc3bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
